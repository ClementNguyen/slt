{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1RssNHQN_lbhxcafUNRzMwoEg0ORvKNBU","authorship_tag":"ABX9TyM+G8IuSz2NW/VJM1qybW7K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"xCjQzaOOZE1Y","executionInfo":{"status":"ok","timestamp":1609194628390,"user_tz":-60,"elapsed":574,"user":{"displayName":"Clément Nguyen","photoUrl":"https://lh3.googleusercontent.com/-kxlJCRyEfxM/AAAAAAAAAAI/AAAAAAAAAkM/h-kNvz51xhc/s64/photo.jpg","userId":"05774458231308576552"}}},"source":["import os\n","import sys"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"91ElFDjybKDo","executionInfo":{"status":"ok","timestamp":1609194632841,"user_tz":-60,"elapsed":5013,"user":{"displayName":"Clément Nguyen","photoUrl":"https://lh3.googleusercontent.com/-kxlJCRyEfxM/AAAAAAAAAAI/AAAAAAAAAkM/h-kNvz51xhc/s64/photo.jpg","userId":"05774458231308576552"}},"outputId":"00e4b898-c85d-43e1-a94d-15c7500d7ba0"},"source":["# Install dependencies\n","!pip install -r /content/drive/MyDrive/3A/Recvis/slt-master/requirements2.txt"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting portalocker==1.5.2\n","  Downloading https://files.pythonhosted.org/packages/91/db/7bc703c0760df726839e0699b7f78a4d8217fdc9c7fcb1b51b39c5a22a4e/portalocker-1.5.2-py2.py3-none-any.whl\n","Collecting torchtext==0.5.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n","\r\u001b[K     |████▌                           | 10kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 20kB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 30kB 37.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 40kB 29.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 51kB 29.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 61kB 32.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 71kB 27.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 9.8MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.0->-r /content/drive/MyDrive/3A/Recvis/slt-master/requirements2.txt (line 68)) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.0->-r /content/drive/MyDrive/3A/Recvis/slt-master/requirements2.txt (line 68)) (1.19.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.0->-r /content/drive/MyDrive/3A/Recvis/slt-master/requirements2.txt (line 68)) (1.7.0+cu101)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.0->-r /content/drive/MyDrive/3A/Recvis/slt-master/requirements2.txt (line 68)) (4.41.1)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 13.9MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.0->-r /content/drive/MyDrive/3A/Recvis/slt-master/requirements2.txt (line 68)) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.5.0->-r /content/drive/MyDrive/3A/Recvis/slt-master/requirements2.txt (line 68)) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.5.0->-r /content/drive/MyDrive/3A/Recvis/slt-master/requirements2.txt (line 68)) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.5.0->-r /content/drive/MyDrive/3A/Recvis/slt-master/requirements2.txt (line 68)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.5.0->-r /content/drive/MyDrive/3A/Recvis/slt-master/requirements2.txt (line 68)) (2.10)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.5.0->-r /content/drive/MyDrive/3A/Recvis/slt-master/requirements2.txt (line 68)) (3.7.4.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.5.0->-r /content/drive/MyDrive/3A/Recvis/slt-master/requirements2.txt (line 68)) (0.16.0)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.5.0->-r /content/drive/MyDrive/3A/Recvis/slt-master/requirements2.txt (line 68)) (0.8)\n","Installing collected packages: portalocker, sentencepiece, torchtext\n","  Found existing installation: torchtext 0.3.1\n","    Uninstalling torchtext-0.3.1:\n","      Successfully uninstalled torchtext-0.3.1\n","Successfully installed portalocker-1.5.2 sentencepiece-0.1.94 torchtext-0.5.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DOoH0db1ekBk","executionInfo":{"status":"ok","timestamp":1609194646375,"user_tz":-60,"elapsed":18544,"user":{"displayName":"Clément Nguyen","photoUrl":"https://lh3.googleusercontent.com/-kxlJCRyEfxM/AAAAAAAAAAI/AAAAAAAAAkM/h-kNvz51xhc/s64/photo.jpg","userId":"05774458231308576552"}}},"source":["dir = 'drive/My Drive/3A/Recvis/slt-master'\n","sys.path.insert(1, dir)\n","\n","from signjoey.training import train\n","from signjoey.prediction import test\n","from signjoey.data import load_data\n","from signjoey.helpers import load_config"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xxaS1rvxsfkJ"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"rAf6z-M0ZhXd","executionInfo":{"status":"ok","timestamp":1609194646376,"user_tz":-60,"elapsed":18542,"user":{"displayName":"Clément Nguyen","photoUrl":"https://lh3.googleusercontent.com/-kxlJCRyEfxM/AAAAAAAAAAI/AAAAAAAAAkM/h-kNvz51xhc/s64/photo.jpg","userId":"05774458231308576552"}}},"source":["config = \"configs/sign.yaml\"\n","cfg_file=dir+'/'+config\n","gpu_id = \"0\"\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_id"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wm_mJbu4taWn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"186b609c-4c1a-42af-bf03-ae91f3d4a3b8"},"source":["train(cfg_file=cfg_file)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2020-12-28 22:31:33,424 Hello! This is Joey-NMT.\n","2020-12-28 22:31:33,444 Total params: 26114111\n","2020-12-28 22:31:33,447 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.lut.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']\n","2020-12-28 22:31:47,787 cfg.name                           : sign_experiment\n","2020-12-28 22:31:47,788 cfg.data.data_path                 : drive/My Drive/3A/Recvis/slt-master/data/\n","2020-12-28 22:31:47,792 cfg.data.version                   : phoenix_2014_trans\n","2020-12-28 22:31:47,793 cfg.data.sgn                       : sign\n","2020-12-28 22:31:47,794 cfg.data.txt                       : text\n","2020-12-28 22:31:47,795 cfg.data.gls                       : gloss\n","2020-12-28 22:31:47,796 cfg.data.train                     : PHOENIX2014T/phoenix14t.pami0.train\n","2020-12-28 22:31:47,797 cfg.data.dev                       : PHOENIX2014T/phoenix14t.pami0.dev\n","2020-12-28 22:31:47,798 cfg.data.test                      : PHOENIX2014T/phoenix14t.pami0.test\n","2020-12-28 22:31:47,799 cfg.data.feature_size              : 1024\n","2020-12-28 22:31:47,800 cfg.data.level                     : word\n","2020-12-28 22:31:47,801 cfg.data.txt_lowercase             : True\n","2020-12-28 22:31:47,802 cfg.data.max_sent_length           : 400\n","2020-12-28 22:31:47,803 cfg.data.random_train_subset       : -1\n","2020-12-28 22:31:47,804 cfg.data.random_dev_subset         : -1\n","2020-12-28 22:31:47,805 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n","2020-12-28 22:31:47,806 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n","2020-12-28 22:31:47,808 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]\n","2020-12-28 22:31:47,808 cfg.training.reset_best_ckpt       : False\n","2020-12-28 22:31:47,809 cfg.training.reset_scheduler       : False\n","2020-12-28 22:31:47,810 cfg.training.reset_optimizer       : False\n","2020-12-28 22:31:47,810 cfg.training.random_seed           : 42\n","2020-12-28 22:31:47,811 cfg.training.model_dir             : drive/My Drive/3A/Recvis/slt-master/sign_sample_model\n","2020-12-28 22:31:47,812 cfg.training.recognition_loss_weight : 1.0\n","2020-12-28 22:31:47,812 cfg.training.translation_loss_weight : 1.0\n","2020-12-28 22:31:47,813 cfg.training.eval_metric           : bleu\n","2020-12-28 22:31:47,814 cfg.training.optimizer             : adam\n","2020-12-28 22:31:47,815 cfg.training.learning_rate         : 0.001\n","2020-12-28 22:31:47,815 cfg.training.batch_size            : 32\n","2020-12-28 22:31:47,816 cfg.training.num_valid_log         : 5\n","2020-12-28 22:31:47,817 cfg.training.epochs                : 5000000\n","2020-12-28 22:31:47,818 cfg.training.early_stopping_metric : eval_metric\n","2020-12-28 22:31:47,818 cfg.training.batch_type            : sentence\n","2020-12-28 22:31:47,819 cfg.training.translation_normalization : batch\n","2020-12-28 22:31:47,820 cfg.training.eval_recognition_beam_size : 1\n","2020-12-28 22:31:47,821 cfg.training.eval_translation_beam_size : 1\n","2020-12-28 22:31:47,821 cfg.training.eval_translation_beam_alpha : -1\n","2020-12-28 22:31:47,822 cfg.training.overwrite             : True\n","2020-12-28 22:31:47,823 cfg.training.shuffle               : True\n","2020-12-28 22:31:47,824 cfg.training.use_cuda              : True\n","2020-12-28 22:31:47,824 cfg.training.translation_max_output_length : 30\n","2020-12-28 22:31:47,825 cfg.training.keep_last_ckpts       : 1\n","2020-12-28 22:31:47,826 cfg.training.batch_multiplier      : 1\n","2020-12-28 22:31:47,826 cfg.training.logging_freq          : 100\n","2020-12-28 22:31:47,827 cfg.training.validation_freq       : 100\n","2020-12-28 22:31:47,828 cfg.training.betas                 : [0.9, 0.998]\n","2020-12-28 22:31:47,829 cfg.training.scheduling            : plateau\n","2020-12-28 22:31:47,829 cfg.training.learning_rate_min     : 1e-07\n","2020-12-28 22:31:47,830 cfg.training.weight_decay          : 0.001\n","2020-12-28 22:31:47,831 cfg.training.patience              : 8\n","2020-12-28 22:31:47,832 cfg.training.decrease_factor       : 0.7\n","2020-12-28 22:31:47,832 cfg.training.label_smoothing       : 0.0\n","2020-12-28 22:31:47,833 cfg.model.initializer              : xavier\n","2020-12-28 22:31:47,834 cfg.model.bias_initializer         : zeros\n","2020-12-28 22:31:47,835 cfg.model.init_gain                : 1.0\n","2020-12-28 22:31:47,836 cfg.model.embed_initializer        : xavier\n","2020-12-28 22:31:47,836 cfg.model.embed_init_gain          : 1.0\n","2020-12-28 22:31:47,837 cfg.model.tied_softmax             : False\n","2020-12-28 22:31:47,838 cfg.model.encoder.type             : transformer\n","2020-12-28 22:31:47,838 cfg.model.encoder.num_layers       : 3\n","2020-12-28 22:31:47,839 cfg.model.encoder.num_heads        : 8\n","2020-12-28 22:31:47,840 cfg.model.encoder.embeddings.embedding_dim : 512\n","2020-12-28 22:31:47,841 cfg.model.encoder.embeddings.scale : False\n","2020-12-28 22:31:47,841 cfg.model.encoder.embeddings.dropout : 0.1\n","2020-12-28 22:31:47,842 cfg.model.encoder.embeddings.norm_type : batch\n","2020-12-28 22:31:47,843 cfg.model.encoder.embeddings.activation_type : softsign\n","2020-12-28 22:31:47,843 cfg.model.encoder.hidden_size      : 512\n","2020-12-28 22:31:47,844 cfg.model.encoder.ff_size          : 2048\n","2020-12-28 22:31:47,845 cfg.model.encoder.dropout          : 0.1\n","2020-12-28 22:31:47,845 cfg.model.decoder.type             : transformer\n","2020-12-28 22:31:47,846 cfg.model.decoder.num_layers       : 3\n","2020-12-28 22:31:47,847 cfg.model.decoder.num_heads        : 8\n","2020-12-28 22:31:47,847 cfg.model.decoder.embeddings.embedding_dim : 512\n","2020-12-28 22:31:47,848 cfg.model.decoder.embeddings.scale : False\n","2020-12-28 22:31:47,849 cfg.model.decoder.embeddings.dropout : 0.1\n","2020-12-28 22:31:47,850 cfg.model.decoder.embeddings.norm_type : batch\n","2020-12-28 22:31:47,850 cfg.model.decoder.embeddings.activation_type : softsign\n","2020-12-28 22:31:47,851 cfg.model.decoder.hidden_size      : 512\n","2020-12-28 22:31:47,852 cfg.model.decoder.ff_size          : 2048\n","2020-12-28 22:31:47,852 cfg.model.decoder.dropout          : 0.1\n","2020-12-28 22:31:47,853 Data set sizes: \n","\ttrain 7095,\n","\tvalid 519,\n","\ttest 642\n","2020-12-28 22:31:47,854 First training example:\n","\t[GLS] JETZT WETTER MORGEN DONNERSTAG ZWOELF FEBRUAR\n","\t[TXT] und nun die wettervorhersage für morgen donnerstag den zwölften august .\n","2020-12-28 22:31:47,854 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) REGEN (4) REGION (5) IX (6) KOMMEN (7) MORGEN (8) NORD (9) SONNE\n","2020-12-28 22:31:47,855 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) und (6) im (7) es (8) der (9) am\n","2020-12-28 22:31:47,856 Number of unique glosses (types): 1087\n","2020-12-28 22:31:47,857 Number of unique words (types): 2889\n","2020-12-28 22:31:47,857 SignModel(\n","\tencoder=TransformerEncoder(num_layers=3, num_heads=8),\n","\tdecoder=TransformerDecoder(num_layers=3, num_heads=8),\n","\tsgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=1024),\n","\ttxt_embed=Embeddings(embedding_dim=512, vocab_size=2889))\n","2020-12-28 22:31:47,871 EPOCH 1\n","/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:359: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  var = torch.tensor(arr, dtype=self.dtype, device=device)\n"],"name":"stderr"},{"output_type":"stream","text":["MODEL sgn torch.Size([32, 177, 1024])\n","MODEL sgn torch.Size([32, 148, 1024])\n","MODEL sgn torch.Size([32, 61, 1024])\n","MODEL sgn torch.Size([32, 104, 1024])\n","MODEL sgn torch.Size([32, 123, 1024])\n","MODEL sgn torch.Size([32, 45, 1024])\n","MODEL sgn torch.Size([32, 65, 1024])\n","MODEL sgn torch.Size([32, 131, 1024])\n","MODEL sgn torch.Size([32, 168, 1024])\n","MODEL sgn torch.Size([32, 101, 1024])\n","MODEL sgn torch.Size([32, 38, 1024])\n","MODEL sgn torch.Size([32, 142, 1024])\n","MODEL sgn torch.Size([32, 187, 1024])\n","MODEL sgn torch.Size([32, 158, 1024])\n","MODEL sgn torch.Size([32, 86, 1024])\n","MODEL sgn torch.Size([32, 104, 1024])\n","MODEL sgn torch.Size([32, 85, 1024])\n","MODEL sgn torch.Size([32, 84, 1024])\n","MODEL sgn torch.Size([32, 98, 1024])\n","MODEL sgn torch.Size([32, 97, 1024])\n","MODEL sgn torch.Size([32, 133, 1024])\n","MODEL sgn torch.Size([32, 102, 1024])\n","MODEL sgn torch.Size([32, 119, 1024])\n","MODEL sgn torch.Size([32, 68, 1024])\n","MODEL sgn torch.Size([32, 96, 1024])\n","MODEL sgn torch.Size([32, 76, 1024])\n","MODEL sgn torch.Size([32, 77, 1024])\n","MODEL sgn torch.Size([32, 156, 1024])\n","MODEL sgn torch.Size([32, 66, 1024])\n","MODEL sgn torch.Size([32, 34, 1024])\n","MODEL sgn torch.Size([32, 130, 1024])\n","MODEL sgn torch.Size([32, 201, 1024])\n","MODEL sgn torch.Size([32, 170, 1024])\n","MODEL sgn torch.Size([32, 106, 1024])\n","MODEL sgn torch.Size([32, 152, 1024])\n","MODEL sgn torch.Size([32, 146, 1024])\n","MODEL sgn torch.Size([32, 74, 1024])\n","MODEL sgn torch.Size([32, 124, 1024])\n","MODEL sgn torch.Size([32, 107, 1024])\n","MODEL sgn torch.Size([32, 113, 1024])\n","MODEL sgn torch.Size([32, 128, 1024])\n","MODEL sgn torch.Size([32, 173, 1024])\n","MODEL sgn torch.Size([32, 260, 1024])\n","MODEL sgn torch.Size([32, 58, 1024])\n","MODEL sgn torch.Size([32, 42, 1024])\n","MODEL sgn torch.Size([32, 144, 1024])\n","MODEL sgn torch.Size([32, 371, 1024])\n","MODEL sgn torch.Size([32, 79, 1024])\n","MODEL sgn torch.Size([32, 129, 1024])\n","MODEL sgn torch.Size([32, 89, 1024])\n","MODEL sgn torch.Size([32, 40, 1024])\n","MODEL sgn torch.Size([32, 53, 1024])\n","MODEL sgn torch.Size([32, 137, 1024])\n","MODEL sgn torch.Size([32, 92, 1024])\n","MODEL sgn torch.Size([32, 105, 1024])\n","MODEL sgn torch.Size([32, 111, 1024])\n","MODEL sgn torch.Size([32, 47, 1024])\n","MODEL sgn torch.Size([32, 161, 1024])\n","MODEL sgn torch.Size([32, 195, 1024])\n","MODEL sgn torch.Size([32, 209, 1024])\n","MODEL sgn torch.Size([32, 88, 1024])\n","MODEL sgn torch.Size([32, 164, 1024])\n","MODEL sgn torch.Size([32, 236, 1024])\n","MODEL sgn torch.Size([32, 112, 1024])\n","MODEL sgn torch.Size([32, 80, 1024])\n","MODEL sgn torch.Size([32, 117, 1024])\n","MODEL sgn torch.Size([32, 115, 1024])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LT_20ofCsoqw"},"source":["# Testing"]},{"cell_type":"code","metadata":{"id":"KOCKKXBflpZg"},"source":["ckpt = dir+'/'+'saved_model/tmp_test.ckpt'\n","output_path = dir+'/'+'saved_model'\n","\n","test(cfg_file, ckpt=ckpt, output_path=output_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"066JmZoJsrqC"},"source":["# Data"]},{"cell_type":"code","metadata":{"id":"RRp1f6LSs1D7"},"source":["cfg = load_config(cfg_file)\n","\n","train_data, dev_data, test_data, gls_vocab, txt_vocab = load_data(\n","    data_cfg=cfg[\"data\"]\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SI1nAcOxt3XM"},"source":["dev_data.fields"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6DypQb5Ev1y7"},"source":["dev_data[0].sgn[0].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tCF1bLwQwNBD"},"source":["dev_data[0].sequence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jd9moJHP2kXZ"},"source":["import torch \r\n","\r\n","a = torch.zeros((1,10,20))\r\n","b = torch.zeros((1,10,40))\r\n","\r\n","torch.cat([a,b], dim=2).shape\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KXFGaduy8dYQ"},"source":["c = a.new(10,10).fill_(1)\r\n","c"],"execution_count":null,"outputs":[]}]}